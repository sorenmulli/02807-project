{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d640b6-eb7f-4c0f-8e2b-610b0e4c3c98",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1536b-ca39-4bd6-8002-42ec9c7fc579",
   "metadata": {},
   "source": [
    "TODO: Insert comments in this part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434ad6f-0297-45f0-83ce-b13168a6f51f",
   "metadata": {},
   "source": [
    "## Get the list of candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba542d0-a9b2-48e5-86ca-0715b205bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import fitz\n",
    "import re\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "# Concatenate hyphenated words\n",
    "TEXT_FLAGS = fitz.TEXT_DEHYPHENATE\n",
    "\n",
    "\n",
    "def extract_text_from_doc(doc):\n",
    "    \"\"\"Extracts text from fitz.Document\n",
    "\n",
    "    Args:\n",
    "        doc (fitz.Document): fitz.Document to extract text from\n",
    "\n",
    "    Returns:\n",
    "        doc_text(List[str]): List of strings where string is cleaned text from page in the doc.\n",
    "    \"\"\"\n",
    "    # Get raw text from doc for every page\n",
    "    page_text = (page.get_text(\"text\", flags=TEXT_FLAGS) for page in doc)\n",
    "    # Split every page_text into list of strings where there are \\n.\n",
    "    page_text = (text.split(\"\\n\") for text in page_text)\n",
    "    # Remove leading and trailing space of every string in list.\n",
    "    page_text = (list((string.strip() for string in text)) for text in page_text)\n",
    "    # Remove empty strings\n",
    "    page_text = (list(filter(None, text)) for text in page_text)\n",
    "    # Concatenate all strings\n",
    "    page_text = (\" \".join(text) for text in page_text)\n",
    "    doc_text = list(page_text)\n",
    "    return doc_text\n",
    "\n",
    "\n",
    "def find_ext(dr, ext):\n",
    "    return glob(path.join(dr, \"*.{}\".format(ext)))\n",
    "\n",
    "\n",
    "file_paths = find_ext(\"candidates/\", \"pdf\")\n",
    "\n",
    "\n",
    "def get_data(file_path):\n",
    "    \"\"\"Extracts text from fitz.Document\n",
    "\n",
    "    Args:\n",
    "        file_path (str): File path to pdf.\n",
    "\n",
    "    Returns:\n",
    "        party_df (pd.DataFrame): Dataframe with candate data and party affiliation.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(file_path)  # Open PDF\n",
    "    text = extract_text_from_doc(doc)  # Extract text from all pages\n",
    "    text.pop(0)\n",
    "    text = \"\\n\".join(text)\n",
    "\n",
    "    ## CLEAN DOCUMENT ##\n",
    "    text = text.replace(\n",
    "        \"(Prioriteret sideordnet opstilling anmeldt i følgende opstillingskredse: Alle) (Valg på personlige stemmer anmeldt)\",\n",
    "        \"\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Alle\",\n",
    "        \"\",\n",
    "    )\n",
    "    text = \"\".join([i for i in text if not i.isdigit()])\n",
    "    text = text.replace(\n",
    "        \"Kandidaternes navne på stemmesedlen     Opstillet i opstillingskreds nr.\",\n",
    "        \"\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"A. Socialdemokratiet \",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"V. Venstre, Danmarks Liberale Parti\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Å. Alternativet\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Ø. Enhedslisten - De Rød-Grønne\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Æ. Danmarksdemokraterne - Inger Støjberg\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Q. Frie Grønne, Danmarks Nye Venstrefløjsparti\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"M. Moderaterne\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"I. Liberal Alliance\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"C. Det Konservative Folkeparti\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"F. SF - Socialistisk Folkeparti\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"K. KD - Kristendemokraterne\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"O. Dansk Folkeparti\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"B. Radikale Venstre\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"D. Nye Borgerlige\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Uden for partierne\",\n",
    "        \"PARTI,\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"(Prioriteret sideordnet opstilling anmeldt i følgende opstillingskredse: )\",\n",
    "        \"\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"(Partiliste anmeldt) .\",\n",
    "        \"\",\n",
    "    )\n",
    "    text = re.sub(\"\\s\\s+\", \",\", text)\n",
    "    text = text.replace(\n",
    "        \",,\",\n",
    "        \",\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Internal - KMD A/S . \",\n",
    "        \"\",\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Internal - KMD A/S \",\n",
    "        \"\",\n",
    "    )\n",
    "\n",
    "    text_party = text.split(\"PARTI\")\n",
    "\n",
    "    parties = []\n",
    "    for i in range(len(text_party)):\n",
    "        text_element = text_party[i].split(\",\")\n",
    "        text_element = [text_ele.strip() for text_ele in text_element]\n",
    "\n",
    "        while \"\" in text_element:\n",
    "            text_element.remove(\"\")\n",
    "\n",
    "        for i, text_ele in enumerate(text_element):\n",
    "            if text_ele[0:2] == \". \":\n",
    "                text_element[i] = text_ele[2:]\n",
    "\n",
    "        parties.append(text_element)\n",
    "\n",
    "    parties.pop(0)\n",
    "\n",
    "    # List of parties\n",
    "    party_list = [\n",
    "        \"A\",\n",
    "        \"B\",\n",
    "        \"C\",\n",
    "        \"D\",\n",
    "        \"F\",\n",
    "        \"I\",\n",
    "        \"K\",\n",
    "        \"M\",\n",
    "        \"O\",\n",
    "        \"Q\",\n",
    "        \"V\",\n",
    "        \"Æ\",\n",
    "        \"Ø\",\n",
    "        \"Å\",\n",
    "        \"UDEN\",\n",
    "    ]\n",
    "\n",
    "    candidate_dict = {\"Candidate\": [], \"Party\": []}\n",
    "    for (candidates, party) in zip(parties, party_list):\n",
    "        for candidate in candidates:\n",
    "            candidate_dict[\"Candidate\"].append(candidate)\n",
    "            candidate_dict[\"Party\"].append(party)\n",
    "\n",
    "    party_df = pd.DataFrame.from_dict(candidate_dict)\n",
    "\n",
    "    return party_df\n",
    "\n",
    "\n",
    "final_df = pd.DataFrame.from_dict({\"Candidate\": [], \"Party\": []})\n",
    "\n",
    "# Make dataframe for all pdf files.\n",
    "for i in range(len(file_paths)):\n",
    "    party_df = get_data(file_paths[i])\n",
    "    final_df = final_df.append(party_df, ignore_index=True)\n",
    "# %%\n",
    "final_df = final_df.drop_duplicates()\n",
    "final_df.to_csv(\"all_candidates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd86550-bba2-4f61-ad70-e3aabc3e9882",
   "metadata": {},
   "source": [
    "## Scrape Twitter users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08c59e-7c11-4278-9ba1-d14a9df57b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from lxml import html\n",
    "\n",
    "from pelutils import log, LogLevels\n",
    "\n",
    "SEARCH_URL = \"https://twitter.com/search?q={0}&src=unknown&f=user\"\n",
    "DRIVER_FOLDER = \".\"\n",
    "os.environ[\"PATH\"] += \":\" + DRIVER_FOLDER\n",
    "\n",
    "def search_user(name: str, driver: webdriver) -> tuple[str, str, str]:\n",
    "    log(\"Getting twitter info for %s\" % name)\n",
    "    driver.get(SEARCH_URL.format(name))\n",
    "    time.sleep(7)  # Wait for JS to load :-P\n",
    "    tree = html.fromstring(driver.page_source)\n",
    "    allres = list()\n",
    "    m = True\n",
    "    i = 1\n",
    "    while m:\n",
    "        m = tree.xpath(f\"/html/body/div[1]/div/div/div[2]/main/div/div/div/div[1]/div/div[3]/div/section/div/div/div[{i}]/div/div/div/div/div[2]\")\n",
    "        i += 1\n",
    "        allres.extend(m)\n",
    "    for res in allres:\n",
    "        try:\n",
    "            name_div, bio_div = list(res)\n",
    "            name_div = list(list(name_div)[0])[0]\n",
    "            display_div, handle_div = list(name_div)\n",
    "            name = display_div[0][0][0][0][0].text\n",
    "            handle = handle_div[0][0][0][0][0].text\n",
    "            bio = bio_div[0].text\n",
    "            bio = \"\"\n",
    "            for bio_elem in bio_div:\n",
    "                try:\n",
    "                    # Links split up the text\n",
    "                    bio += bio_elem[0][0].text\n",
    "                except IndexError:\n",
    "                    bio += bio_elem.text\n",
    "            log(\"Got data for %s with bio\" % handle, with_info=False)\n",
    "        except ValueError:\n",
    "            # No bio (probably, at least in one case)\n",
    "            name_div = res[0]\n",
    "            name_div = list(list(name_div)[0])[0]\n",
    "            display_div, handle_div = list(name_div)\n",
    "            name = display_div[0][0][0][0][0].text\n",
    "            handle = handle_div[0][0][0][0][0].text\n",
    "            bio = \"\"\n",
    "            log(\"Got data %s with no bio\" % handle, with_info=False)\n",
    "        return name, handle, bio\n",
    "\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    log.configure(\"scrape.log\", print_level=LogLevels.DEBUG)\n",
    "\n",
    "    data = list()\n",
    "\n",
    "with open(\"data/all_candidates.csv\") as fp_cand, open(\"data/candidates_full.csv\", \"w\") as fp_full:\n",
    "    reader = csv.reader(fp_cand, delimiter=\",\")\n",
    "    writer = csv.writer(fp_full, delimiter=\",\", quoting=csv.QUOTE_MINIMAL)\n",
    "    fails = list()\n",
    "    writer.writerow((\"Name\", \"Handle\", \"Party\", \"Bio\"))\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        name, party_letter = row\n",
    "        try:\n",
    "            name, handle, bio = search_user(name, driver)\n",
    "        except TypeError:\n",
    "            log.error(\"Failed to get data for %s\" % name)\n",
    "            fails.append(name)\n",
    "        writer.writerow((name, handle, party_letter, bio))\n",
    "        fp_full.flush()  # Force write to file during run\n",
    "    log(\"Failed to get info for the following\", *fails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedbe50-9b9f-418c-8e0b-828853173b6c",
   "metadata": {},
   "source": [
    "## Get tweets from Twitters API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbe8ee-d779-46c0-9012-f8ab805226cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "\n",
    "STD_PATH = os.path.join(os.path.dirname(sys.argv[0]), \"..\", \"secrets.json\")\n",
    "\n",
    "\n",
    "def get_client(secret_path: str = STD_PATH):\n",
    "    try:\n",
    "        with open(secret_path, \"r\") as f:\n",
    "            secrets = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            \"HEY! You don't have any secrets! You must get them secretly from Søren\"\n",
    "        )\n",
    "\n",
    "    return tweepy.Client(secrets[\"bearer\"], wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "def get_ids(client: tweepy.Client, usernames: List[str]) -> List[Tuple[int, str]]:\n",
    "    BATCH_SIZE = 75\n",
    "    SLEEP = 5\n",
    "    res = list()\n",
    "    num_batches = int(np.ceil(len(usernames) / BATCH_SIZE))\n",
    "    for i in range(num_batches):\n",
    "        print(f\"{i}/{num_batches-1}\")\n",
    "        batch = usernames[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "        try:\n",
    "            res.extend([r[\"id\"] for r in client.get_users(usernames=batch).data])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed! with {e}\")\n",
    "            res.append([None for _ in batch])\n",
    "        time.sleep(SLEEP)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_user_tweets(client: tweepy.Client, user_id: str):\n",
    "    END = datetime.fromisoformat(\"2022-11-02\")\n",
    "    START = datetime.fromisoformat(\"2022-10-04\")\n",
    "\n",
    "    tweets = list()\n",
    "    res = client.get_users_tweets(user_id, max_results=100)\n",
    "    if res.data:\n",
    "        tweets.extend(res.data)\n",
    "    while res.meta.get(\"next_token\") and len(res.data):\n",
    "        res = client.get_users_tweets(\n",
    "            user_id,\n",
    "            pagination_token=res.meta[\"next_token\"],\n",
    "            max_results=100,\n",
    "            start_time=START,\n",
    "            end_time=END,\n",
    "        )\n",
    "        if res.data:\n",
    "            tweets.extend(res.data)\n",
    "    return [(t.id, t.text) for t in tweets] if tweets else []\n",
    "\n",
    "\n",
    "def get_all_user_tweets(client: tweepy.Client, user_ids: List[str]) -> pd.DataFrame:\n",
    "    SLEEP = 2\n",
    "\n",
    "    failed_ids = list()\n",
    "    tweets, tids, uids = list(), list(), list()\n",
    "    for i, uid in enumerate(user_ids):\n",
    "        print(f\"{i}/{len(user_ids)-1}\")\n",
    "        try:\n",
    "            utweets = get_user_tweets(client, uid)\n",
    "            for (tid, ttxt) in utweets:\n",
    "                tids.append(tid)\n",
    "                tweets.append(ttxt)\n",
    "                uids.append(uid)\n",
    "        except Exception as e:\n",
    "            print(e, uid)\n",
    "            failed_ids.append(uid)\n",
    "        time.sleep(SLEEP)\n",
    "    print(\"pls retry:\", failed_ids)\n",
    "    return pd.DataFrame(dict(tweetID=tids, userID=uids, tweet=tweets))\n",
    "\n",
    "\n",
    "client = get_client()\n",
    "df = pd.read_csv(\"data/candidates_full.csv\")\n",
    "df[\"id\"] = get_ids(client, [h.replace(\"@\", \"\") for h in df.Handle])\n",
    "df.to_csv(\"data/candidates_with_id.csv\")\n",
    "\n",
    "df = pd.read_csv(\"data/candidates_with_id.csv\")\n",
    "df_tweet = get_all_user_tweets(client, df[\"id\"])\n",
    "df_tweet.to_csv(\"data/tweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
